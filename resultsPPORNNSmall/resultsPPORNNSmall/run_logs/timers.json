{
    "name": "root",
    "gauges": {
        "Hunter.Policy.Entropy.mean": {
            "value": 1.0439069271087646,
            "min": 1.0439069271087646,
            "max": 1.4189382791519165,
            "count": 186
        },
        "Hunter.Policy.Entropy.sum": {
            "value": 11700.1083984375,
            "min": 9201.185546875,
            "max": 202700.78125,
            "count": 186
        },
        "Hunter.Environment.EpisodeLength.mean": {
            "value": 173.93103448275863,
            "min": 93.56603773584905,
            "max": 220.77272727272728,
            "count": 186
        },
        "Hunter.Environment.EpisodeLength.sum": {
            "value": 10088.0,
            "min": 9398.0,
            "max": 143086.0,
            "count": 186
        },
        "Hunter.Self-play.ELO.mean": {
            "value": 1438.5069075686627,
            "min": 1218.2771482283522,
            "max": 1506.7671359317158,
            "count": 186
        },
        "Hunter.Self-play.ELO.sum": {
            "value": 83433.40063898244,
            "min": 62879.35439104362,
            "max": 145131.1288356546,
            "count": 186
        },
        "Hunter.Step.mean": {
            "value": 1859946.0,
            "min": 9955.0,
            "max": 1859946.0,
            "count": 186
        },
        "Hunter.Step.sum": {
            "value": 1859946.0,
            "min": 9955.0,
            "max": 1859946.0,
            "count": 186
        },
        "Hunter.Policy.ExtrinsicValueEstimate.mean": {
            "value": 1.34846830368042,
            "min": -0.14768217504024506,
            "max": 1.4325650930404663,
            "count": 186
        },
        "Hunter.Policy.ExtrinsicValueEstimate.sum": {
            "value": 76.8626937866211,
            "min": -15.506628036499023,
            "max": 130.85435485839844,
            "count": 186
        },
        "Hunter.Environment.CumulativeReward.mean": {
            "value": 0.4245614017310895,
            "min": -0.6949495039503983,
            "max": 0.6071428531514746,
            "count": 186
        },
        "Hunter.Environment.CumulativeReward.sum": {
            "value": 24.199999898672104,
            "min": -68.80000089108944,
            "max": 37.199999928474426,
            "count": 186
        },
        "Hunter.Policy.ExtrinsicReward.mean": {
            "value": 3.073684209271481,
            "min": 2.2193548390942235,
            "max": 3.548648644138027,
            "count": 186
        },
        "Hunter.Policy.ExtrinsicReward.sum": {
            "value": 175.19999992847443,
            "min": 110.19999974966049,
            "max": 296.5999997854233,
            "count": 186
        },
        "Hunter.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 186
        },
        "Hunter.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 186
        },
        "Hunter.Losses.PolicyLoss.mean": {
            "value": 0.0902447242476046,
            "min": 0.07115774844074622,
            "max": 0.21666151873767375,
            "count": 108
        },
        "Hunter.Losses.PolicyLoss.sum": {
            "value": 0.0902447242476046,
            "min": 0.07115774844074622,
            "max": 0.21666151873767375,
            "count": 108
        },
        "Hunter.Losses.ValueLoss.mean": {
            "value": 0.13673738554120063,
            "min": 0.11127423368394375,
            "max": 0.24926930963993071,
            "count": 108
        },
        "Hunter.Losses.ValueLoss.sum": {
            "value": 0.13673738554120063,
            "min": 0.11127423368394375,
            "max": 0.24926930963993071,
            "count": 108
        },
        "Hunter.Policy.LearningRate.mean": {
            "value": 0.0003,
            "min": 0.0003,
            "max": 0.0003,
            "count": 108
        },
        "Hunter.Policy.LearningRate.sum": {
            "value": 0.0003,
            "min": 0.0003,
            "max": 0.0003,
            "count": 108
        },
        "Hunter.Policy.Epsilon.mean": {
            "value": 0.19999999999999996,
            "min": 0.19999999999999996,
            "max": 0.19999999999999996,
            "count": 108
        },
        "Hunter.Policy.Epsilon.sum": {
            "value": 0.19999999999999996,
            "min": 0.19999999999999996,
            "max": 0.19999999999999996,
            "count": 108
        },
        "Hunter.Policy.Beta.mean": {
            "value": 0.005,
            "min": 0.005,
            "max": 0.005,
            "count": 108
        },
        "Hunter.Policy.Beta.sum": {
            "value": 0.005,
            "min": 0.005,
            "max": 0.005,
            "count": 108
        },
        "Prey.Policy.Entropy.mean": {
            "value": 1.2718061208724976,
            "min": 1.2718061208724976,
            "max": 1.4214280843734741,
            "count": 180
        },
        "Prey.Policy.Entropy.sum": {
            "value": 12087.2451171875,
            "min": 7710.986328125,
            "max": 444522.28125,
            "count": 180
        },
        "Prey.Environment.EpisodeLength.mean": {
            "value": 182.0,
            "min": 99.42424242424242,
            "max": 218.13333333333333,
            "count": 180
        },
        "Prey.Environment.EpisodeLength.sum": {
            "value": 9828.0,
            "min": 8658.0,
            "max": 308595.0,
            "count": 180
        },
        "Prey.Self-play.ELO.mean": {
            "value": 1276.0012202456055,
            "min": 1059.4908642217015,
            "max": 1298.6863648179883,
            "count": 180
        },
        "Prey.Self-play.ELO.sum": {
            "value": 68904.0658932627,
            "min": 57283.89440336057,
            "max": 110121.10493819225,
            "count": 180
        },
        "Prey.Step.mean": {
            "value": 1799762.0,
            "min": 9969.0,
            "max": 1799762.0,
            "count": 180
        },
        "Prey.Step.sum": {
            "value": 1799762.0,
            "min": 9969.0,
            "max": 1799762.0,
            "count": 180
        },
        "Prey.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": -0.23276644945144653,
            "min": -2.6980271339416504,
            "max": 0.11457373946905136,
            "count": 180
        },
        "Prey.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": -12.802154541015625,
            "min": -208.19239807128906,
            "max": 6.4161295890808105,
            "count": 180
        },
        "Prey.Policy.ExtrinsicValueEstimate.mean": {
            "value": -0.19516101479530334,
            "min": -2.724385976791382,
            "max": 0.13164280354976654,
            "count": 180
        },
        "Prey.Policy.ExtrinsicValueEstimate.sum": {
            "value": -10.733856201171875,
            "min": -228.0538787841797,
            "max": 7.371996879577637,
            "count": 180
        },
        "Prey.Environment.CumulativeReward.mean": {
            "value": 1.0201307039369236,
            "min": -1.2310526742943024,
            "max": 1.727272056360194,
            "count": 180
        },
        "Prey.Environment.CumulativeReward.sum": {
            "value": 56.1071887165308,
            "min": -116.95000405795872,
            "max": 82.40438682353124,
            "count": 180
        },
        "Prey.Policy.ExtrinsicReward.mean": {
            "value": 0.6748724742369219,
            "min": -6.800957867973729,
            "max": 3.1022506597194264,
            "count": 180
        },
        "Prey.Policy.ExtrinsicReward.sum": {
            "value": 37.1179860830307,
            "min": -646.0909974575043,
            "max": 145.80578100681305,
            "count": 180
        },
        "Prey.Environment.GroupCumulativeReward.mean": {
            "value": -2.5454545454545454,
            "min": -3.5074626865671643,
            "max": -1.5555555555555556,
            "count": 180
        },
        "Prey.Environment.GroupCumulativeReward.sum": {
            "value": -140.0,
            "min": -308.0,
            "max": -91.0,
            "count": 180
        },
        "Prey.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 180
        },
        "Prey.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 180
        },
        "Prey.Losses.PolicyLoss.mean": {
            "value": 0.08802766134031118,
            "min": 0.07325191061633328,
            "max": 0.3179234877228737,
            "count": 102
        },
        "Prey.Losses.PolicyLoss.sum": {
            "value": 0.08802766134031118,
            "min": 0.07325191061633328,
            "max": 0.3179234877228737,
            "count": 102
        },
        "Prey.Losses.ValueLoss.mean": {
            "value": 0.28863628059625623,
            "min": 0.1441913125415643,
            "max": 0.7432459274927775,
            "count": 102
        },
        "Prey.Losses.ValueLoss.sum": {
            "value": 0.28863628059625623,
            "min": 0.1441913125415643,
            "max": 0.7432459274927775,
            "count": 102
        },
        "Prey.Losses.BaselineLoss.mean": {
            "value": 0.33872220913569134,
            "min": 0.16155933414896329,
            "max": 1.6990285873413087,
            "count": 102
        },
        "Prey.Losses.BaselineLoss.sum": {
            "value": 0.33872220913569134,
            "min": 0.16155933414896329,
            "max": 1.6990285873413087,
            "count": 102
        },
        "Prey.Policy.LearningRate.mean": {
            "value": 0.0003,
            "min": 0.0003,
            "max": 0.0003,
            "count": 102
        },
        "Prey.Policy.LearningRate.sum": {
            "value": 0.0003,
            "min": 0.0003,
            "max": 0.0003,
            "count": 102
        },
        "Prey.Policy.Epsilon.mean": {
            "value": 0.20000000000000007,
            "min": 0.20000000000000007,
            "max": 0.20000000000000007,
            "count": 102
        },
        "Prey.Policy.Epsilon.sum": {
            "value": 0.20000000000000007,
            "min": 0.20000000000000007,
            "max": 0.20000000000000007,
            "count": 102
        },
        "Prey.Policy.Beta.mean": {
            "value": 0.005000000000000001,
            "min": 0.005000000000000001,
            "max": 0.005000000000000001,
            "count": 102
        },
        "Prey.Policy.Beta.sum": {
            "value": 0.005000000000000001,
            "min": 0.005000000000000001,
            "max": 0.005000000000000001,
            "count": 102
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1746732285",
        "python_version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\caggi\\OneDrive\\Documents\\GitHub\\IHM-RL-Project\\venv\\Scripts\\mlagents-learn Assets/config/HunterPreyPPORNNSmall.yml --run-id=resultsPPORNNSmall --results-dir=resultsPPORNNSmall --force",
        "mlagents_version": "1.1.0",
        "mlagents_envs_version": "1.1.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.1.2+cu121",
        "numpy_version": "1.23.5",
        "end_time_seconds": "1746736367"
    },
    "total": 4081.7502924000146,
    "count": 1,
    "self": 10.009591199923307,
    "children": {
        "run_training.setup": {
            "total": 0.09828510007355362,
            "count": 1,
            "self": 0.09828510007355362
        },
        "TrainerController.start_learning": {
            "total": 4071.6424161000177,
            "count": 1,
            "self": 2.6614618281600997,
            "children": {
                "TrainerController._reset_env": {
                    "total": 25.152483600191772,
                    "count": 19,
                    "self": 25.152483600191772
                },
                "TrainerController.advance": {
                    "total": 4043.3704865716863,
                    "count": 137250,
                    "self": 2.9197852994548157,
                    "children": {
                        "env_step": {
                            "total": 2867.48053079308,
                            "count": 137250,
                            "self": 2170.404682785971,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 695.2814482848626,
                                    "count": 137250,
                                    "self": 20.74627641646657,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 674.535171868396,
                                            "count": 258952,
                                            "self": 674.535171868396
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 1.7943997222464532,
                                    "count": 137249,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 4035.8342953309184,
                                            "count": 137249,
                                            "is_parallel": true,
                                            "self": 2307.973590601003,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.01497050013858825,
                                                    "count": 38,
                                                    "is_parallel": true,
                                                    "self": 0.0026364002842456102,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.01233409985434264,
                                                            "count": 76,
                                                            "is_parallel": true,
                                                            "self": 0.01233409985434264
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 1727.845734229777,
                                                    "count": 137249,
                                                    "is_parallel": true,
                                                    "self": 39.01904065941926,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 52.91529378062114,
                                                            "count": 137249,
                                                            "is_parallel": true,
                                                            "self": 52.91529378062114
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 1545.7159229066456,
                                                            "count": 137249,
                                                            "is_parallel": true,
                                                            "self": 1545.7159229066456
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 90.19547688309103,
                                                            "count": 274498,
                                                            "is_parallel": true,
                                                            "self": 17.755300889140926,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 72.4401759939501,
                                                                    "count": 548996,
                                                                    "is_parallel": true,
                                                                    "self": 72.4401759939501
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 1172.9701704791514,
                            "count": 274498,
                            "self": 30.69198616198264,
                            "children": {
                                "process_trajectory": {
                                    "total": 787.324723617523,
                                    "count": 274498,
                                    "self": 786.3160011173459,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 1.0087225001771003,
                                            "count": 6,
                                            "self": 1.0087225001771003
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 354.9534606996458,
                                    "count": 210,
                                    "self": 43.70961249945685,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 116.98344280128367,
                                            "count": 5400,
                                            "self": 116.98344280128367
                                        },
                                        "TorchPOCAOptimizer.update": {
                                            "total": 194.26040539890528,
                                            "count": 3060,
                                            "self": 194.26040539890528
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 1.100008375942707e-06,
                    "count": 1,
                    "self": 1.100008375942707e-06
                },
                "TrainerController._save_models": {
                    "total": 0.45798299997113645,
                    "count": 1,
                    "self": 0.006753599853254855,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.4512294001178816,
                            "count": 2,
                            "self": 0.4512294001178816
                        }
                    }
                }
            }
        }
    }
}